{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97fead0b",
   "metadata": {},
   "source": [
    "# EXTRACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5e693d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 09:01:11,392 - INFO - \n",
      " ----------------------------------------------------------------------------------\n",
      "2025-08-22 09:01:11,392 - INFO - \n",
      " Executando Rotina - RELATORIO ATIVACOES PENDENTES\n",
      "2025-08-22 09:01:11,414 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:01:13,314 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:01:14,726 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:01:17,081 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:01:18,002 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:01:21,001 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:01:22,933 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:01:24,802 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:01:27,697 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:01:29,530 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:01:31,349 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:01:34,383 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:01:36,151 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:01:37,997 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:01:40,920 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:01:42,733 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:01:44,566 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:01:47,542 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:01:49,396 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:01:51,232 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:01:54,593 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:01:56,470 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:01:58,711 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:02:00,762 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:02:01,816 - INFO - Created CTAS table \"silver\".\"temp_table_3366d7d183874636a6cb73e7532eba78\"\n",
      "2025-08-22 09:02:01,831 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:02:04,825 - INFO - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-08-22 09:02:04,848 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:02:06,463 - INFO - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-08-22 09:02:06,498 - INFO - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-08-22 09:02:06,529 - INFO - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-08-22 09:02:06,530 - INFO - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-08-22 09:02:07,136 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:02:09,462 - INFO - \n",
      " ----------------------------------------------------------------------------------\n",
      "2025-08-22 09:02:09,478 - INFO - \n",
      " Relatorio ativacoes (Vivante)  - Dados Extraidos com sucesso!\n",
      "2025-08-22 09:02:09,492 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:02:11,478 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:02:13,896 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:02:16,350 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:02:17,418 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:02:19,541 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:02:21,711 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:02:25,143 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:02:27,049 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:02:29,184 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:02:32,182 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:02:34,165 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:02:36,126 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:02:39,152 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:02:40,983 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:02:42,888 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:02:46,126 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:02:47,090 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:02:48,036 - INFO - Created CTAS table \"silver\".\"temp_table_494c5da0113c4213ba490fa80fe395ba\"\n",
      "2025-08-22 09:02:48,047 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:02:50,275 - INFO - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-08-22 09:02:50,288 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:02:52,183 - INFO - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-08-22 09:02:52,239 - INFO - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-08-22 09:02:52,397 - INFO - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-08-22 09:02:52,405 - INFO - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-08-22 09:02:52,868 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:02:55,477 - INFO - \n",
      " ----------------------------------------------------------------------------------\n",
      "2025-08-22 09:02:55,477 - INFO - \n",
      " Relatorio cancelamentos  - Dados Extraidos com sucesso!\n",
      "2025-08-22 09:02:55,500 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:02:56,531 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:02:59,248 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:03:01,667 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:03:02,685 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:03:04,743 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:03:06,687 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:03:09,626 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:03:11,483 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:03:13,514 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:03:16,695 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:03:19,727 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:03:22,670 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:03:24,525 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:03:26,443 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:03:29,354 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:03:31,166 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:03:33,028 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:03:36,030 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:03:37,851 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:03:39,682 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:03:41,582 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:03:42,366 - INFO - Created CTAS table \"silver\".\"temp_table_60e208a267f041c7bd1a3525781feda0\"\n",
      "2025-08-22 09:03:42,375 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:03:44,291 - INFO - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-08-22 09:03:44,304 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:03:45,662 - INFO - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-08-22 09:03:45,671 - INFO - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-08-22 09:03:45,742 - INFO - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-08-22 09:03:45,767 - INFO - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-08-22 09:03:47,702 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-22 09:03:50,057 - INFO - \n",
      " ----------------------------------------------------------------------------------\n",
      "2025-08-22 09:03:50,058 - INFO - \n",
      " Relatorio conferência  - Dados Extraidos com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import awswrangler as awr\n",
    "import logging\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Exibe mensagens a partir de INFO\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler()  # Garante logs no console\n",
    "    ]\n",
    ")\n",
    "\n",
    "logging.info('\\n ----------------------------------------------------------------------------------')\n",
    "logging.info('\\n Executando Rotina - RELATORIO ATIVACOES PENDENTES')\n",
    "\n",
    "class Extract:\n",
    "\n",
    "    # FUNÇÃO CONSTRUTORA\n",
    "    def __init__(self):\n",
    "        #self.path = os.path.dirname(__file__) #atributo de instância que obtém o caminho do diretório dirname\n",
    "        self.path = r\"C:\\Users\\raphael.almeida\\Documents\\Processos\\placas_acompanhamento\"\n",
    "\n",
    "    # FUNÇÃO PARA EXTRAÇÃO DE DADOS DE: ATIVAÇÕES\n",
    "    def extract_all_ativacoes(self):\n",
    "        \"\"\"\n",
    "        Retorna um DataFrame com os dados de ativações.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            dir_query = os.path.join(self.path, 'sql', 'all_boards_ATIVOS.sql')\n",
    "            with open(dir_query, 'r') as file:\n",
    "                query = file.read()\n",
    "            df_ativacoes = awr.athena.read_sql_query(query, database='silver')\n",
    "            logging.info('\\n ----------------------------------------------------------------------------------')\n",
    "            logging.info('\\n Relatorio ativacoes (Vivante)  - Dados Extraidos com sucesso!')\n",
    "            return df_ativacoes\n",
    "        except Exception as e:\n",
    "            logging.info('\\n ----------------------------------------------------------------------------------')\n",
    "            logging.info(f'\\n Falha ao Extrair relatorio ativacoes (Viavante): {e}')\n",
    "            return None\n",
    "\n",
    "    # FUNÇÃO PARA EXTRAÇÃO DE DADOS DE: CANCELAMENTO INTEGRAL\n",
    "    def extract_all_cancelamentos(self):\n",
    "        \"\"\"\n",
    "        Retorna um DataFrame com os dados de cancelamentos.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            dir_query = os.path.join(self.path, 'sql', 'all_boards_CANCELADOS.sql')\n",
    "            with open(dir_query, 'r') as file:\n",
    "                query = file.read()\n",
    "            df_cancelamentos = awr.athena.read_sql_query(query, database='silver')\n",
    "            logging.info('\\n ----------------------------------------------------------------------------------')\n",
    "            logging.info('\\n Relatorio cancelamentos  - Dados Extraidos com sucesso!')\n",
    "            return df_cancelamentos\n",
    "        except Exception as e:\n",
    "            logging.info('\\n ----------------------------------------------------------------------------------')\n",
    "            logging.info(f'\\n Falha ao Extrair relatorio cancelamentos: {e}')\n",
    "            return None\n",
    "\n",
    "    # FUNÇÃO PARA EXTRAÇÃO DE DADOS DE: CONFERÊNCIA\n",
    "    def extract_conf_boards(self):\n",
    "        \"\"\"\n",
    "        Retorna um DataFrame com os dados de conferência.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            dir_query = os.path.join(self.path, 'sql', 'listagem_mestra.sql')\n",
    "            with open(dir_query, 'r') as file:\n",
    "                query = file.read()\n",
    "            df_conferencia = awr.athena.read_sql_query(query, database='silver')\n",
    "            logging.info('\\n ----------------------------------------------------------------------------------')\n",
    "            logging.info('\\n Relatorio conferência  - Dados Extraidos com sucesso!')\n",
    "            return df_conferencia\n",
    "        except Exception as e:\n",
    "            logging.info('\\n ----------------------------------------------------------------------------------')\n",
    "            logging.info(f'\\n Falha ao Extrair relatorio conferência: {e}')\n",
    "            return None\n",
    "\n",
    "# Exemplo de como declarar as variáveis dos returns dos métodos para usar depois:\n",
    "# Basta instanciar a classe e chamar os métodos, atribuindo o retorno a variáveis:\n",
    "\n",
    "extract = Extract()\n",
    "df_ativacoes = extract.extract_all_ativacoes()\n",
    "df_cancelamentos = extract.extract_all_cancelamentos()\n",
    "df_conferencia = extract.extract_conf_boards()\n",
    "\n",
    "# Agora você pode usar df_ativacoes, df_cancelamentos, df_conferencia normalmente no seu código.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1d536d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>placa</th>\n",
       "      <th>chassi</th>\n",
       "      <th>id_placa</th>\n",
       "      <th>id_veiculo</th>\n",
       "      <th>id_carroceria</th>\n",
       "      <th>matricula</th>\n",
       "      <th>conjunto</th>\n",
       "      <th>unidade</th>\n",
       "      <th>consultor</th>\n",
       "      <th>status</th>\n",
       "      <th>...</th>\n",
       "      <th>empresa</th>\n",
       "      <th>coverage</th>\n",
       "      <th>beneficio</th>\n",
       "      <th>categoria</th>\n",
       "      <th>tipo_categoria</th>\n",
       "      <th>data_ativacao_beneficio</th>\n",
       "      <th>status_beneficio</th>\n",
       "      <th>data_atualizacao</th>\n",
       "      <th>consultor_ativo</th>\n",
       "      <th>id_beneficio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MKC0G31</td>\n",
       "      <td>9534N8243BR157808</td>\n",
       "      <td>1651</td>\n",
       "      <td>1651</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>277</td>\n",
       "      <td>274</td>\n",
       "      <td>UNIDADE JOINVILLE</td>\n",
       "      <td>Ana Beatriz Prioste</td>\n",
       "      <td>ATIVO</td>\n",
       "      <td>...</td>\n",
       "      <td>Tag</td>\n",
       "      <td>678515</td>\n",
       "      <td>Rastreador p/ Veículo</td>\n",
       "      <td>PADRÃO</td>\n",
       "      <td>PADRÃO</td>\n",
       "      <td>2025-08-06</td>\n",
       "      <td>ATIVO</td>\n",
       "      <td>2025-08-15</td>\n",
       "      <td>S</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BTO1021</td>\n",
       "      <td>9BM6931084B375803</td>\n",
       "      <td>1676</td>\n",
       "      <td>1676</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>278</td>\n",
       "      <td>277</td>\n",
       "      <td>UNIDADE CAMPO GRANDE</td>\n",
       "      <td>Daiane Cristina Veiga da Silva</td>\n",
       "      <td>ATIVO</td>\n",
       "      <td>...</td>\n",
       "      <td>Tag</td>\n",
       "      <td>678592</td>\n",
       "      <td>Assistência a Reparos (COMPLEMENTO)</td>\n",
       "      <td>CARROCERIA BAÚ</td>\n",
       "      <td>PADRÃO - 4%</td>\n",
       "      <td>2025-08-08</td>\n",
       "      <td>ATIVO</td>\n",
       "      <td>2025-08-15</td>\n",
       "      <td>S</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IKK6B93</td>\n",
       "      <td>9EP07082021000077</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>UNIDADE CASCAVEL</td>\n",
       "      <td>Thiago Rosa De Figueiredo</td>\n",
       "      <td>ATIVO</td>\n",
       "      <td>...</td>\n",
       "      <td>Tag</td>\n",
       "      <td>676370</td>\n",
       "      <td>Rastreador p/ Reboque</td>\n",
       "      <td>PADRÃO</td>\n",
       "      <td>PADRÃO</td>\n",
       "      <td>2025-08-06</td>\n",
       "      <td>ATIVO</td>\n",
       "      <td>2025-08-13</td>\n",
       "      <td>S</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BDE7E35</td>\n",
       "      <td>9BVRG40D3LE870664</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>UNIDADE CASCAVEL</td>\n",
       "      <td>Thiago Rosa De Figueiredo</td>\n",
       "      <td>ATIVO</td>\n",
       "      <td>...</td>\n",
       "      <td>Tag</td>\n",
       "      <td>676367</td>\n",
       "      <td>Rastreador p/ Veículo</td>\n",
       "      <td>PADRÃO</td>\n",
       "      <td>PADRÃO</td>\n",
       "      <td>2025-08-06</td>\n",
       "      <td>ATIVO</td>\n",
       "      <td>2025-08-13</td>\n",
       "      <td>S</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BDE7E35</td>\n",
       "      <td>9BVRG40D3LE870664</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>UNIDADE CASCAVEL</td>\n",
       "      <td>Thiago Rosa De Figueiredo</td>\n",
       "      <td>ATIVO</td>\n",
       "      <td>...</td>\n",
       "      <td>Tag</td>\n",
       "      <td>676366</td>\n",
       "      <td>APP</td>\n",
       "      <td>PADRÃO</td>\n",
       "      <td>PADRÃO</td>\n",
       "      <td>2025-08-06</td>\n",
       "      <td>ATIVO</td>\n",
       "      <td>2025-08-13</td>\n",
       "      <td>S</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     placa             chassi  id_placa  id_veiculo  id_carroceria  matricula  \\\n",
       "0  MKC0G31  9534N8243BR157808      1651        1651           <NA>        277   \n",
       "1  BTO1021  9BM6931084B375803      1676        1676           <NA>        278   \n",
       "2  IKK6B93  9EP07082021000077        19           0             19          2   \n",
       "3  BDE7E35  9BVRG40D3LE870664        19          19           <NA>          2   \n",
       "4  BDE7E35  9BVRG40D3LE870664        19          19           <NA>          2   \n",
       "\n",
       "   conjunto               unidade                       consultor status  ...  \\\n",
       "0       274     UNIDADE JOINVILLE             Ana Beatriz Prioste  ATIVO  ...   \n",
       "1       277  UNIDADE CAMPO GRANDE  Daiane Cristina Veiga da Silva  ATIVO  ...   \n",
       "2         4      UNIDADE CASCAVEL       Thiago Rosa De Figueiredo  ATIVO  ...   \n",
       "3         4      UNIDADE CASCAVEL       Thiago Rosa De Figueiredo  ATIVO  ...   \n",
       "4         4      UNIDADE CASCAVEL       Thiago Rosa De Figueiredo  ATIVO  ...   \n",
       "\n",
       "  empresa coverage                            beneficio       categoria  \\\n",
       "0     Tag   678515                Rastreador p/ Veículo          PADRÃO   \n",
       "1     Tag   678592  Assistência a Reparos (COMPLEMENTO)  CARROCERIA BAÚ   \n",
       "2     Tag   676370                Rastreador p/ Reboque          PADRÃO   \n",
       "3     Tag   676367                Rastreador p/ Veículo          PADRÃO   \n",
       "4     Tag   676366                                  APP          PADRÃO   \n",
       "\n",
       "  tipo_categoria data_ativacao_beneficio  status_beneficio data_atualizacao  \\\n",
       "0         PADRÃO              2025-08-06             ATIVO       2025-08-15   \n",
       "1    PADRÃO - 4%              2025-08-08             ATIVO       2025-08-15   \n",
       "2         PADRÃO              2025-08-06             ATIVO       2025-08-13   \n",
       "3         PADRÃO              2025-08-06             ATIVO       2025-08-13   \n",
       "4         PADRÃO              2025-08-06             ATIVO       2025-08-13   \n",
       "\n",
       "  consultor_ativo id_beneficio  \n",
       "0               S           44  \n",
       "1               S           39  \n",
       "2               S           45  \n",
       "3               S           44  \n",
       "4               S           48  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ativacoes[df_ativacoes['empresa'] == 'Tag'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4563010",
   "metadata": {},
   "source": [
    "# TRANSFORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3eb57fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raphael.almeida\\AppData\\Local\\Temp\\ipykernel_14572\\778490356.py:105: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df_ativacoes = df_ativacoes.loc[df_ativacoes['beneficio'].str.contains('(CASCO|TERCEIRO|Assistência a reparos)', regex=True, case = False)]\n",
      "2025-08-22 09:48:47,038 - INFO - \n",
      " ----------------------------------------------------------------------------------\n",
      "2025-08-22 09:48:47,038 - INFO - Shape do DataFrame antes do tratamento: (368, 26)\n",
      "C:\\Users\\raphael.almeida\\AppData\\Local\\Temp\\ipykernel_14572\\778490356.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['status_beneficio'] = 'NOVO'\n",
      "C:\\Users\\raphael.almeida\\AppData\\Local\\Temp\\ipykernel_14572\\778490356.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['migration_from'] = np.nan\n",
      "C:\\Users\\raphael.almeida\\AppData\\Local\\Temp\\ipykernel_14572\\778490356.py:71: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Viavante' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[mask, 'migration_from'] = empresa_penultima\n",
      "2025-08-22 09:49:05,252 - INFO - \n",
      " ----------------------------------------------------------------------------------\n",
      "2025-08-22 09:49:05,252 - INFO - Processamento concluído com sucesso!\n",
      "2025-08-22 09:49:05,451 - INFO - \n",
      " ----------------------------------------------------------------------------------\n",
      "2025-08-22 09:49:05,451 - INFO - Número de registros ativos na carteira tratado com os dados do dia anterior.\n",
      "2025-08-22 09:49:05,522 - INFO - \n",
      " ----------------------------------------------------------------------------------\n",
      "2025-08-22 09:49:05,522 - INFO - Processo final de tratamento de dataframe de ativação realizado com sucesso!\n",
      "2025-08-22 09:49:05,557 - INFO - \n",
      " ----------------------------------------------------------------------------------\n",
      "2025-08-22 09:49:05,557 - INFO - \n",
      " Processo de Transformacao de Dados concluido com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANDO MÓDULOS E PACOTES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Exibe mensagens a partir de INFO\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler()  # Garante logs no console\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def board_status_treatment(df, df_conf):\n",
    "\n",
    "    try:\n",
    "        status_filter_list = ['CANCELADO', 'CANCELADA', 'FINALIZADO', 'FINALIZADA', 'NAO RENOVADO']\n",
    "\n",
    "        if df is None or df.empty:\n",
    "            logging.info('\\n ----------------------------------------------------------------------------------')\n",
    "            logging.info('DataFrame vazio, retornando DataFrame vazio')\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        if df_conf is None or df_conf.empty:\n",
    "            logging.info('\\n ----------------------------------------------------------------------------------')\n",
    "            logging.info('DataFrame de conferência vazio, retornando DataFrame original')\n",
    "            return df\n",
    "\n",
    "        required_columns = ['placa', 'chassi', 'empresa', 'status']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            logging.info('\\n ----------------------------------------------------------------------------------')\n",
    "            logging.info(f'Colunas necessárias ausentes no DataFrame: {missing_columns}')\n",
    "            return df\n",
    "\n",
    "        logging.info('\\n ----------------------------------------------------------------------------------')\n",
    "        logging.info(f'Shape do DataFrame antes do tratamento: {df.shape}')\n",
    "\n",
    "        df['status_beneficio'] = 'NOVO'\n",
    "        df['migration_from'] = np.nan\n",
    "\n",
    "        df_conf_grouped = df_conf.groupby(['chassi', 'beneficio']).agg({\n",
    "            'empresa': list,\n",
    "            'data_ativacao_beneficio': list,\n",
    "            'status_beneficio': list\n",
    "        }).reset_index()\n",
    "\n",
    "        conf_dict = df_conf_grouped.set_index(['chassi', 'beneficio']).to_dict('index')\n",
    "\n",
    "        for chassi, beneficio in df[['chassi', 'beneficio']].drop_duplicates().values:\n",
    "            if (chassi, beneficio) in conf_dict:\n",
    "                conf_data = conf_dict[(chassi, beneficio)]\n",
    "\n",
    "                if len(conf_data['empresa']) > 1:\n",
    "                    dates = sorted([d for d in conf_data['data_ativacao_beneficio'] if pd.notna(d)])\n",
    "\n",
    "                    if len(dates) > 1:\n",
    "                        penultima_data = dates[-2]\n",
    "                        idx_penultima = conf_data['data_ativacao_beneficio'].index(penultima_data)\n",
    "                        status_penultimo = conf_data['status_beneficio'][idx_penultima]\n",
    "                        empresa_penultima = conf_data['empresa'][idx_penultima]\n",
    "\n",
    "                        mask = (df['chassi'] == chassi) & (df['beneficio'] == beneficio)\n",
    "\n",
    "                        if status_penultimo not in status_filter_list:\n",
    "                            if empresa_penultima != df.loc[mask, 'empresa'].iloc[0]:\n",
    "                                df.loc[mask, 'status_beneficio'] = 'MIGRAÇÃO'\n",
    "                                df.loc[mask, 'migration_from'] = empresa_penultima\n",
    "                            else:\n",
    "                                df.loc[mask, 'status_beneficio'] = 'RENOVAÇÃO'\n",
    "                        else:\n",
    "                            df.loc[mask, 'status_beneficio'] = 'REATIVAÇÃO'\n",
    "\n",
    "        logging.info('\\n ----------------------------------------------------------------------------------')\n",
    "        logging.info(f'Processamento concluído com sucesso!')\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.info('\\n ----------------------------------------------------------------------------------')\n",
    "        logging.info(f'Falha no tratamento de status das placas ativadas: {str(e)}')\n",
    "        return df\n",
    "\n",
    "\n",
    "try:\n",
    "    today = dt.date.today()\n",
    "    yesterday = today - dt.timedelta(days=1)\n",
    "    friday = today - dt.timedelta(days=3)\n",
    "    df_final_ativacoes = pd.DataFrame()\n",
    "\n",
    "    # Tratamento dos benefícios e filtragem\n",
    "    try:\n",
    "        df_ativacoes['data_ativacao_beneficio'] = pd.to_datetime(df_ativacoes['data_ativacao_beneficio']).dt.date\n",
    "        df_ativacoes['beneficio'] = df_ativacoes['beneficio'].replace(\n",
    "            'REPARAÇÃO OU REPOSIÇÃO DO VEÍCULO', 'CASCO (VEÍCULO)'\n",
    "        ).replace(\n",
    "            'REPARAÇÃO OU REPOSIÇÃO DO (SEMI)REBOQUE', 'CASCO (R/SR)'\n",
    "        ).replace(\n",
    "            'REPARAÇÃO OU REPOSIÇÃO DO COMPLEMENTO', 'CASCO (COMPLEMENTO)'\n",
    "        )\n",
    "        df_ativacoes['migration_from'] = np.nan\n",
    "        df_ativacoes = df_ativacoes.loc[df_ativacoes['beneficio'].str.contains('(CASCO|TERCEIRO|Assistência a reparos)', regex=True, case = False)]\n",
    "\n",
    "        df_conferencia['beneficio'] = df_conferencia['beneficio'].replace(\n",
    "            'REPARAÇÃO OU REPOSIÇÃO DO VEÍCULO', 'CASCO (VEÍCULO)'\n",
    "        ).replace(\n",
    "            'REPARAÇÃO OU REPOSIÇÃO DO (SEMI)REBOQUE', 'CASCO (R/SR)'\n",
    "        ).replace(\n",
    "            'REPARAÇÃO OU REPOSIÇÃO DO COMPLEMENTO', 'CASCO (COMPLEMENTO)'\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logging.info('\\n ----------------------------------------------------------------------------------')\n",
    "        logging.info(f'Falha ao filtrar dados de cancelamentos referente ao dia anterior: {e}')\n",
    "\n",
    "    # Cancelamentos do dia anterior ou desde sexta\n",
    "    try:\n",
    "        df_cancelamentos['data_cancelamento'] = pd.to_datetime(df_cancelamentos['data_cancelamento']).dt.date\n",
    "        if today.weekday() == 0:\n",
    "            df_cancelamentos = df_cancelamentos[df_cancelamentos['data_cancelamento'].between(friday, today)]\n",
    "        else:\n",
    "            df_cancelamentos = df_cancelamentos[df_cancelamentos['data_cancelamento'] == yesterday]\n",
    "\n",
    "        df_cancelamentos = df_cancelamentos.sort_values(by='data_cancelamento', ascending=True)\n",
    "    except Exception as e:\n",
    "        logging.info('\\n ----------------------------------------------------------------------------------')\n",
    "        logging.info(f'Falha ao filtrar dados de cancelamentos referente ao dia anterior: {e}')\n",
    "\n",
    "    # Ativações do dia anterior ou desde sexta\n",
    "    try:\n",
    "        if not df_ativacoes.empty:\n",
    "            if today.weekday() != 0:\n",
    "                df_ativacoes_menos_ontem = df_ativacoes.loc[\n",
    "                    ~(df_ativacoes['data_ativacao_beneficio'].isin([yesterday, today]))\n",
    "                ]\n",
    "                df_ativacoes_dia_anterior = df_ativacoes[df_ativacoes['data_ativacao_beneficio'] == yesterday]\n",
    "            else:\n",
    "                df_ativacoes_menos_ontem = df_ativacoes.loc[\n",
    "                    (df_ativacoes['data_ativacao_beneficio'] < friday)\n",
    "                ]\n",
    "                df_ativacoes_dia_anterior = df_ativacoes[\n",
    "                    df_ativacoes['data_ativacao_beneficio'].between(friday, yesterday)\n",
    "                ]\n",
    "\n",
    "            df_ativacoes_dia_anterior_tratado = board_status_treatment(\n",
    "                df=df_ativacoes_dia_anterior, df_conf=df_conferencia\n",
    "            )\n",
    "            df_ativacoes_atualizado = pd.concat([df_ativacoes_menos_ontem, df_ativacoes_dia_anterior_tratado])\n",
    "\n",
    "            logging.info('\\n ----------------------------------------------------------------------------------')\n",
    "            logging.info(f'Número de registros ativos na carteira tratado com os dados do dia anterior.')\n",
    "    except Exception as e:\n",
    "        logging.info('\\n ----------------------------------------------------------------------------------')\n",
    "        logging.info(f'Falha ao incluir registros ativos referente ao dia anterior na contagem . Revise o código: {e}')\n",
    "\n",
    "    # Último tratamento do DataFrame de ativação\n",
    "    try:\n",
    "        df_final_ativacoes = df_ativacoes_atualizado[[\n",
    "            'placa', 'chassi', 'id_placa', 'id_veiculo', 'id_carroceria', 'matricula', 'conjunto', 'unidade',\n",
    "            'consultor', 'status_beneficio', 'cliente', 'data_registro', 'data_ativacao_beneficio', 'suporte',\n",
    "            'data_filtro', 'empresa', 'migration_from'\n",
    "        ]]\n",
    "        df_final_ativacoes = df_final_ativacoes.drop_duplicates(subset='chassi')\n",
    "\n",
    "        logging.info('\\n ----------------------------------------------------------------------------------')\n",
    "        logging.info(f'Processo final de tratamento de dataframe de ativação realizado com sucesso!')\n",
    "    except Exception as e:\n",
    "        logging.info('\\n ----------------------------------------------------------------------------------')\n",
    "        print(f'Falha ao tratar o dataframe de ativação: {e}')\n",
    "\n",
    "    # Tratando dados nulos nos DataFrames\n",
    "    try:\n",
    "        df_final_ativacoes['placa'] = df_final_ativacoes['placa'].fillna('SEM-PLACA')\n",
    "        df_final_ativacoes['chassi'] = df_final_ativacoes['chassi'].fillna('NULL')\n",
    "        df_final_ativacoes['id_placa'] = df_final_ativacoes['id_placa'].fillna(0)\n",
    "        df_final_ativacoes['id_veiculo'] = df_final_ativacoes['id_veiculo'].fillna(0)\n",
    "        df_final_ativacoes['id_carroceria'] = df_final_ativacoes['id_carroceria'].fillna(0)\n",
    "        df_final_ativacoes['matricula'] = df_final_ativacoes['matricula'].fillna(0)\n",
    "        df_final_ativacoes['conjunto'] = df_final_ativacoes['conjunto'].fillna(0)\n",
    "        df_final_ativacoes['unidade'] = df_final_ativacoes['unidade'].fillna('NULL')\n",
    "        df_final_ativacoes['consultor'] = df_final_ativacoes['consultor'].fillna('NULL')\n",
    "        df_final_ativacoes['status'] = df_final_ativacoes.get('status', pd.Series(['NULL']*len(df_final_ativacoes))).fillna('NULL')\n",
    "        df_final_ativacoes['cliente'] = df_final_ativacoes['cliente'].fillna('NULL')\n",
    "        df_final_ativacoes['data_registro'] = df_final_ativacoes['data_registro'].fillna(pd.Timestamp('1900-01-01'))\n",
    "        if 'data_ativacao' in df_final_ativacoes.columns:\n",
    "            df_final_ativacoes['data_ativacao'] = df_final_ativacoes['data_ativacao'].fillna(pd.Timestamp('1900-01-01'))\n",
    "        df_final_ativacoes['suporte'] = df_final_ativacoes['suporte'].fillna('NULL')\n",
    "        df_final_ativacoes['data_filtro'] = df_final_ativacoes['data_filtro'].fillna(pd.Timestamp('1900-01-01'))\n",
    "        df_final_ativacoes['empresa'] = df_final_ativacoes['empresa'].fillna('NULL')\n",
    "\n",
    "        df_cancelamentos['placa'] = df_cancelamentos['placa'].fillna('SEM-PLACA')\n",
    "        df_cancelamentos['chassi'] = df_cancelamentos['chassi'].fillna('NULL')\n",
    "        df_cancelamentos['id_placa'] = df_cancelamentos['id_placa'].fillna(0)\n",
    "        df_cancelamentos['id_veiculo'] = df_cancelamentos['id_veiculo'].fillna(0)\n",
    "        df_cancelamentos['id_carroceria'] = df_cancelamentos['id_carroceria'].fillna(0)\n",
    "        df_cancelamentos['matricula'] = df_cancelamentos['matricula'].fillna(0)\n",
    "        df_cancelamentos['conjunto'] = df_cancelamentos['conjunto'].fillna(0)\n",
    "        df_cancelamentos['unidade'] = df_cancelamentos['unidade'].fillna('NULL')\n",
    "        df_cancelamentos['status'] = df_cancelamentos['status'].fillna('NULL')\n",
    "        df_cancelamentos['cliente'] = df_cancelamentos['cliente'].fillna('NULL')\n",
    "        df_cancelamentos['data'] = df_cancelamentos['data'].fillna(pd.Timestamp('1900-01-01'))\n",
    "        df_cancelamentos['data_cancelamento'] = df_cancelamentos['data_cancelamento'].fillna(pd.Timestamp('1900-01-01'))\n",
    "        df_cancelamentos['usuario_cancelamento'] = df_cancelamentos['usuario_cancelamento'].fillna('NULL')\n",
    "        df_cancelamentos['data_filtro'] = df_cancelamentos['data_filtro'].fillna(pd.Timestamp('1900-01-01'))\n",
    "        df_cancelamentos['empresa'] = df_cancelamentos['empresa'].fillna('NULL')\n",
    "\n",
    "        logging.info('\\n ----------------------------------------------------------------------------------')\n",
    "        logging.info('\\n Processo de Transformacao de Dados concluido com sucesso!')\n",
    "    except Exception as e:\n",
    "        logging.info('\\n ----------------------------------------------------------------------------------')\n",
    "        logging.info(f'Falha ao realizar tratamento de dados: {e}')\n",
    "\n",
    "except Exception as e:\n",
    "    logging.info('\\n ----------------------------------------------------------------------------------')\n",
    "    logging.info(f'Falha ao realizar tratamento de dados: {e}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0da4447f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32830"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_ativacoes.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34762361",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['data'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_final_ativacoes \u001b[38;5;241m=\u001b[39m \u001b[43mdf_ativacoes_atualizado\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mplaca\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchassi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid_placa\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid_veiculo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid_carroceria\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmatricula\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconjunto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43munidade\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconsultor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstatus_beneficio\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcliente\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata_ativacao_beneficio\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msuporte\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata_filtro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mempresa\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmigration_from\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m      5\u001b[0m \u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      6\u001b[0m df_final_ativacoes \u001b[38;5;241m=\u001b[39m df_final_ativacoes\u001b[38;5;241m.\u001b[39mdrop_duplicates(subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchassi\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\raphael.almeida\\Documents\\Workspace Python\\env2\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\raphael.almeida\\Documents\\Workspace Python\\env2\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\raphael.almeida\\Documents\\Workspace Python\\env2\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['data'] not in index\""
     ]
    }
   ],
   "source": [
    "df_final_ativacoes = df_ativacoes_atualizado[[\n",
    "    'placa', 'chassi', 'id_placa', 'id_veiculo', 'id_carroceria', 'matricula', 'conjunto', 'unidade',\n",
    "    'consultor', 'status_beneficio', 'cliente', 'data', 'data_ativacao_beneficio', 'suporte',\n",
    "    'data_filtro', 'empresa', 'migration_from'\n",
    "]]\n",
    "df_final_ativacoes = df_final_ativacoes.drop_duplicates(subset='chassi')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
